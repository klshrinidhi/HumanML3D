{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Poses from Amass Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please remember to download the following subdataset from AMASS website: https://amass.is.tue.mpg.de/download.php. Note only download the <u>SMPL+H G</u> data.\n",
    "* ACCD (ACCD)\n",
    "* HDM05 (MPI_HDM05)\n",
    "* TCDHands (TCD_handMocap)\n",
    "* SFU (SFU)\n",
    "* BMLmovi (BMLmovi)\n",
    "* CMU (CMU)\n",
    "* Mosh (MPI_mosh)\n",
    "* EKUT (EKUT)\n",
    "* KIT  (KIT)\n",
    "* Eyes_Janpan_Dataset (Eyes_Janpan_Dataset)\n",
    "* BMLhandball (BMLhandball)\n",
    "* Transitions (Transitions_mocap)\n",
    "* PosePrior (MPI_Limits)\n",
    "* HumanEva (HumanEva)\n",
    "* SSM (SSM_synced)\n",
    "* DFaust (DFaust_67)\n",
    "* TotalCapture (TotalCapture)\n",
    "* BMLrub (BioMotionLab_NTroje)\n",
    "\n",
    "### Unzip all datasets. In the bracket we give the name of the unzipped file folder. Please correct yours to the given names if they are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place all files under the directory **./amass_data/**. The directory structure shoud look like the following:  \n",
    "./amass_data/  \n",
    "./amass_data/ACCAD/  \n",
    "./amass_data/BioMotionLab_NTroje/  \n",
    "./amass_data/BMLhandball/  \n",
    "./amass_data/BMLmovi/   \n",
    "./amass_data/CMU/  \n",
    "./amass_data/DFaust_67/  \n",
    "./amass_data/EKUT/  \n",
    "./amass_data/Eyes_Japan_Dataset/  \n",
    "./amass_data/HumanEva/  \n",
    "./amass_data/KIT/  \n",
    "./amass_data/MPI_HDM05/  \n",
    "./amass_data/MPI_Limits/  \n",
    "./amass_data/MPI_mosh/  \n",
    "./amass_data/SFU/  \n",
    "./amass_data/SSM_synced/  \n",
    "./amass_data/TCD_handMocap/  \n",
    "./amass_data/TotalCapture/  \n",
    "./amass_data/Transitions_mocap/  \n",
    "\n",
    "**Please make sure the file path are correct, otherwise it can not succeed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "neutral_bm_path = '/home/shrik/HumanML3D/body_models/SMPLX_NEUTRAL_2020.npz'\n",
    "num_betas = 16\n",
    "neutral_bm = BodyModel(bm_fname=neutral_bm_path, num_betas=num_betas, num_expressions=0).to(comp_device)\n",
    "faces = c2c(neutral_bm.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "folders = []\n",
    "dataset_names = []\n",
    "for root, dirs, files in os.walk('./amass_data'):\n",
    "#     print(root, dirs, files)\n",
    "#     for folder in dirs:\n",
    "#         folders.append(os.path.join(root, folder))\n",
    "    folders.append(root)\n",
    "    for name in files:\n",
    "        dataset_name = root.split('/')[2]\n",
    "        if dataset_name not in dataset_names:\n",
    "            dataset_names.append(dataset_name)\n",
    "        if name.endswith('.npz'):\n",
    "            paths.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = './pose_data'\n",
    "save_folders = [folder.replace('./amass_data', './pose_data') for folder in folders]\n",
    "for folder in save_folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "group_path = [[path for path in paths if name in path] for name in dataset_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_matrix = np.array([[1.0, 0.0, 0.0],\n",
    "                         [0.0, 0.0, 1.0],\n",
    "                         [0.0, 1.0, 0.0]])\n",
    "ex_fps = 20\n",
    "def amass_to_pose(src_path, save_path):\n",
    "    bdata = np.load(src_path, allow_pickle=True)\n",
    "    fps = 0\n",
    "    try:\n",
    "        fps = bdata['mocap_frame_rate']\n",
    "    except:\n",
    "        # for k,v in bdata.items():\n",
    "        #     print(k,v.shape)\n",
    "        # print(src_path)\n",
    "        return fps\n",
    "    if bdata['surface_model_type'].item() not in {'smplx','smplx_locked_head'}:\n",
    "        return fps\n",
    "    assert bdata['gender'] == 'neutral'\n",
    "    bm = neutral_bm\n",
    "    down_sample = int(fps / ex_fps)\n",
    "#     print(frame_number)\n",
    "#     print(fps)\n",
    "\n",
    "    bdata_poses = bdata['poses'][::down_sample,...]\n",
    "    bdata_trans = bdata['trans'][::down_sample,...]\n",
    "    body_parms = {\n",
    "            'root_orient': torch.Tensor(bdata_poses[:, :3]).to(comp_device),\n",
    "            'pose_body': torch.Tensor(bdata_poses[:, 3:66]).to(comp_device),\n",
    "            'pose_hand': torch.Tensor(bdata_poses[:, 75:]).to(comp_device),\n",
    "            'trans': torch.Tensor(bdata_trans).to(comp_device),\n",
    "            'betas': torch.Tensor(np.repeat(bdata['betas'][:num_betas][np.newaxis], repeats=len(bdata_trans), axis=0)).to(comp_device),\n",
    "        }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        body = bm(**body_parms)\n",
    "    pose_seq_np = body.Jtr.detach().cpu().numpy()\n",
    "    pose_seq_np_n = np.dot(pose_seq_np, trans_matrix)\n",
    "    \n",
    "    \n",
    "    np.save(save_path, pose_seq_np_n)\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_path = group_path\n",
    "all_count = sum([len(paths) for paths in group_path])\n",
    "cur_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: MoSh: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2049/2049 [00:02<00:00, 928.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 2049/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: CMU: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2079/2079 [00:11<00:00, 186.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 4128/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: KIT: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 4287/4287 [00:19<00:00, 217.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 8415/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: Transitions: 100%|█████████████████████████████████████████████████████████████████████████████████████| 111/111 [00:00<00:00, 112.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 8526/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: GRAB: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1350/1350 [00:00<00:00, 2017.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 9876/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: BMLrub: 100%|█████████████████████████████████████████████████████████████████████████████████████| 3172/3172 [00:00<00:00, 119662.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 13048/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: Eyes_Japan_Dataset: 100%|███████████████████████████████████████████████████████████████████████████| 762/762 [00:00<00:00, 142994.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 13810/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: HDM05: 100%|████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:00<00:00, 135599.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 14029/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: TotalCapture: 100%|████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:00<00:00, 44927.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 14071/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: YOGI_2_latest_smplx_neutral: 100%|██████████████████████████████████████████████████████████████████| 171/171 [00:00<00:00, 153680.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 14242/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: TCDHands: 100%|████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 92618.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 14305/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: WEIZMANN: 100%|███████████████████████████████████████████████████████████████████████████████████| 2227/2227 [00:00<00:00, 175768.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 16532/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: EKUT: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 353/353 [00:00<00:00, 143788.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 16885/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: SSM: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 49556.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 16918/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: SFU: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:00<00:00, 43504.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 16969/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: SOMA: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [00:00<00:00, 88894.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 17040/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: HumanEva: 100%|████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:00<00:00, 46620.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 17071/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: BMLmovi: 100%|████████████████████████████████████████████████████████████████████████████████████| 1953/1953 [00:00<00:00, 100883.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 19024/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: ACCAD: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 272/272 [00:00<00:00, 78387.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 19296/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: CNRS: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 81/81 [00:00<00:00, 100158.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 19377/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: PosePrior: 100%|███████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:00<00:00, 56299.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 19415/19554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: DFaust: 100%|████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:00<00:00, 75626.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed / All (fps 0): 19554/19554\n",
      "bad zip file: ./amass_data/BMLmovi/Subject_49_F_MoSh/Subject_49_F_19_stageii.npz\n",
      "bad zip file: ./amass_data/BMLmovi/Subject_49_F_MoSh/Subject_49_F_19_stageii.npz\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "bad_zip_files = list()\n",
    "unused_files = list()\n",
    "for paths in group_path:\n",
    "    dataset_name = paths[0].split('/')[2]\n",
    "    fps = 0\n",
    "    for path in tqdm(paths,desc='Processing: %s'%dataset_name,ncols=150):\n",
    "        save_path = path.replace('./amass_data', './pose_data')\n",
    "        save_path = save_path[:-3] + 'npy'\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "        try:\n",
    "            fps = amass_to_pose(path, save_path)\n",
    "        except zipfile.BadZipFile:\n",
    "            bad_zip_files.append(path)\n",
    "        except:\n",
    "            unused_files.append(path)\n",
    "    cur_count += len(paths)\n",
    "    print('Processed / All (fps %d): %d/%d'% (fps, cur_count, all_count) )\n",
    "    time.sleep(0.5)\n",
    "for f in bad_zip_files:\n",
    "    print('bad zip file:',f)\n",
    "for f in unused_files:\n",
    "    print('unused file:',f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment, Mirror and Relocate Motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "bm_params_f = pathlib.Path('./body_models/SMPLX_NEUTRAL_2020.npz')\n",
    "index_f = pathlib.Path('./index.csv')\n",
    "pose_data_d  = pathlib.Path('./pose_data')\n",
    "joints_d = pathlib.Path('./joints')\n",
    "fps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the corresponding left/right joints from model npy file. We will mirror left/right joints to augment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_params = np.load(bm_params_f,allow_pickle=True)\n",
    "joint2ind = bm_params['joint2num'].item()\n",
    "ind2joint = {v:k\n",
    "             for k,v in joint2ind.items()}\n",
    "l_joints,r_joints = list(),list()\n",
    "for j in joint2ind:\n",
    "    if j.startswith('L_'):\n",
    "        l_j = j\n",
    "        r_j = j.replace('L_','R_')\n",
    "        l_joints.append(joint2ind[l_j])\n",
    "        r_joints.append(joint2ind[r_j])\n",
    "# print('num joints to swap:',len(l_joints))\n",
    "# for l,r in sorted(zip(l_joints,r_joints)):\n",
    "#     print(ind2joint[l],ind2joint[r])\n",
    "joints_to_drop = [joint2ind['Jaw'],\n",
    "                  joint2ind['L_Eye'],\n",
    "                  joint2ind['R_Eye']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample frames according to HumanML3D, create the dictionary of files to sample, their start/end frames, their original ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sample = defaultdict(list)\n",
    "n_dropped = 0\n",
    "for row in csv.DictReader(open(index_f)):\n",
    "    data_f = row['source_path']\n",
    "    # Not using `humanact12` dataset. Discard those entries in val set.\n",
    "    if 'humanact12' in data_f:\n",
    "        n_dropped += 1\n",
    "        continue\n",
    "    # SMPL-X version of AMASS is missing the following. Ignore them.\n",
    "    if ('BMLhandball' in data_f or\n",
    "        'DanceDB' in data_f or\n",
    "        'HUMAN4D' in data_f or \n",
    "        'CMU/22_23_Rory' in data_f or\n",
    "        'CMU/18_19_rory' in data_f or\n",
    "        'CMU/18_19_Justin' in data_f or\n",
    "        'CMU/20_21_rory1' in data_f or\n",
    "        'CMU/20_21_Justin1' in data_f or\n",
    "        'CMU/22_23_justin' in data_f):\n",
    "        n_dropped += 1\n",
    "        continue\n",
    "    # SMPL-X version of AMASS has renamed many files. Map the old names to new.\n",
    "    data_f = (data_f\n",
    "              .replace('/BioMotionLab_NTroje/','/BMLrub/')\n",
    "              .replace('/DFaust_67/','/DFaust/')\n",
    "              .replace('/MPI_mosh/','/MoSh/')\n",
    "              .replace('/MPI_HDM05/','/HDM05/')\n",
    "              .replace('/MPI_Limits/','/PosePrior/')\n",
    "              .replace('/SSM_synced/','/SSM/')\n",
    "              .replace('/TCD_handMocap/','/TCDHands/')\n",
    "              .replace('/Transitions_mocap/','/Transitions/')\n",
    "              .replace('.npz','')\n",
    "              .replace('.npy','')\n",
    "              .replace('_poses','')\n",
    "              .replace(' ','_'))\n",
    "    data_f = pathlib.Path(data_f)\n",
    "    data_d = data_f.parent\n",
    "    assert data_d.is_dir(),data_d\n",
    "    for f in data_d.iterdir():\n",
    "        if f.name.startswith(data_f.name):\n",
    "            to_sample[f].append((int(row['start_frame']),\n",
    "                                 int(row['end_frame']),\n",
    "                                 row['new_name'].replace('.npy','')))\n",
    "            break\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many files need to be sampled multiple times because there are multiple entries in `index.csv` for those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dropped: 1365\n",
      "files to sample: 10220\n",
      "files with >1 sample: 2459\n",
      "total samples: 13251\n"
     ]
    }
   ],
   "source": [
    "print('samples dropped:',n_dropped)\n",
    "print('files to sample:',len(to_sample))\n",
    "print('files with >1 sample:', \n",
    "      sum(1\n",
    "          for v in to_sample.values()\n",
    "          if len(v) > 1))\n",
    "print('total samples:',\n",
    "      sum(len(v)\n",
    "          for v in to_sample.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num files: 17138\n"
     ]
    }
   ],
   "source": [
    "data_fs = list()\n",
    "for rt,ds,fs in os.walk(pose_data_d):\n",
    "    rt = pathlib.Path(rt)\n",
    "    for f in fs:\n",
    "        f = rt / f\n",
    "        if f.suffix != '.npy':\n",
    "            continue\n",
    "        data_fs.append(f)\n",
    "data_fs.sort()\n",
    "print('num files:',len(data_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample each file as per HumanML3D, create a mirrored version of the sample, save both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mirroring/pruning files:   0%|                                                                                              | 0/17138 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mirroring/pruning files: 100%|█████████████████████████████████████████████████████| 17138/17138 [08:35<00:00, 33.26it/s, samples=26504, dropped=4525]\n"
     ]
    }
   ],
   "source": [
    "n_dropped = 0\n",
    "pose_data_to_joints_map = ['file,new_id,orig_id']\n",
    "pbar = tqdm(data_fs,desc='mirroring/pruning files',ncols=150)\n",
    "for i,f in enumerate(pbar):\n",
    "    if f not in to_sample:\n",
    "        n_dropped += 1\n",
    "        continue\n",
    "    for j,(i_beg,i_end,orig_id) in enumerate(sorted(to_sample[f])):\n",
    "        id = f'{i:06}_{j:02}'\n",
    "        id_m = f'M{i:06}_{j:02}'\n",
    "        out_f = joints_d / f'{id}.npy'\n",
    "        out_m_f = joints_d / f'{id_m}.npy'\n",
    "        data = np.load(f)\n",
    "        if 'humanact12' not in str(f):\n",
    "            if 'Eyes_Japan_Dataset' in str(f):\n",
    "                data = data[3*fps:]\n",
    "            if 'HDM05' in str(f):\n",
    "                data = data[3*fps:]\n",
    "            if 'TotalCapture' in str(f):\n",
    "                data = data[1*fps:]\n",
    "            if 'PosePrior' in str(f):\n",
    "                data = data[1*fps:]\n",
    "            if 'Transitions' in str(f):\n",
    "                data = data[int(0.5*fps):]\n",
    "        data = data[i_beg:i_end]\n",
    "        if out_f.is_file() and out_m_f.is_file():\n",
    "            pose_data_to_joints_map.append(f'{f},{id},{orig_id}')    \n",
    "            continue\n",
    "        data[...,0] *= -1\n",
    "        data_m = data.copy()\n",
    "        data_m[:,l_joints] = data[:,r_joints]\n",
    "        data_m[:,r_joints] = data[:,l_joints]\n",
    "        data = np.delete(data,joints_to_drop,axis=1)\n",
    "        data_m = np.delete(data_m,joints_to_drop,axis=1)\n",
    "        np.save(out_f,data)\n",
    "        np.save(out_m_f,data_m)\n",
    "        pose_data_to_joints_map.append(f'{f},{id},{orig_id}')\n",
    "        pbar.set_postfix({'samples':2*len(pose_data_to_joints_map),\n",
    "                          'dropped':n_dropped})\n",
    "_ = open('pose_data_to_joints_map.txt','w').write('\\n'.join(pose_data_to_joints_map) + '\\n')\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout original train/val/test sets and texts. Just to make sure we have the original versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout main -- HumanML3D/train.txt HumanML3D/val.txt HumanML3D/test.txt HumanML3D/texts.zip\n",
    "!rm -rf HumanML3D/train_orig.txt HumanML3D/val_orig.txt HumanML3D/test_orig.txt HumanML3D/texts_orig HumanML3D/texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "_ = shutil.move('HumanML3D/train.txt','HumanML3D/train_orig.txt')\n",
    "_ = shutil.move('HumanML3D/val.txt','HumanML3D/val_orig.txt')\n",
    "_ = shutil.move('HumanML3D/test.txt','HumanML3D/test_orig.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new test set from original test set with missing samples removed. SMPL-X version of AMASS doesn't seem to have all the original samples from SMPL-H version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig test set: 4384\n",
      "new test set: 3980\n"
     ]
    }
   ],
   "source": [
    "orig_test = [l.strip()\n",
    "             for l in open('./HumanML3D/test_orig.txt')]\n",
    "print('orig test set:',len(orig_test))\n",
    "\n",
    "pose_data_to_joints_map_f = pathlib.Path('pose_data_to_joints_map.txt')\n",
    "orig_id_to_new_id = {row['orig_id']:row['new_id']\n",
    "                     for row in csv.DictReader(open(pose_data_to_joints_map_f))}\n",
    "\n",
    "index_f = pathlib.Path('./index.csv')\n",
    "orig_id_to_orig_file = {row['new_name'].replace('.npy',''):row['source_path']\n",
    "                        for row in csv.DictReader(open(index_f))}\n",
    "\n",
    "new_test = list()\n",
    "for orig_id in orig_test:\n",
    "    is_m_id = orig_id.startswith('M')\n",
    "    if is_m_id:\n",
    "        orig_m_id = orig_id\n",
    "        orig_id = orig_m_id[1:]\n",
    "    else:\n",
    "        orig_m_id = f'M{orig_id}'\n",
    "    data_f = orig_id_to_orig_file[orig_id]\n",
    "    # Not using `humanact12` dataset. Discard those entries in val set.\n",
    "    if 'humanact12' in data_f:\n",
    "        continue\n",
    "    # SMPL-X version of AMASS is missing the following. Ignore them.\n",
    "    if ('BMLhandball' in data_f or\n",
    "        'DanceDB' in data_f or\n",
    "        'HUMAN4D' in data_f or \n",
    "        'CMU/22_23_Rory' in data_f or\n",
    "        'CMU/18_19_rory' in data_f or\n",
    "        'CMU/18_19_Justin' in data_f or\n",
    "        'CMU/20_21_rory1' in data_f or\n",
    "        'CMU/20_21_Justin1' in data_f or\n",
    "        'CMU/22_23_justin' in data_f):\n",
    "        continue\n",
    "    new_test.append(orig_id_to_new_id[orig_id])\n",
    "open('HumanML3D/test.txt','w').write('\\n'.join(sorted(new_test))+'\\n')\n",
    "print('new test set:',len(new_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the remaining samples as the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig train set: 23384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 26502/26502 [00:00<00:00, 413800.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new train set: 24512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "orig_train = [l.strip()\n",
    "             for l in open('./HumanML3D/train_orig.txt')]\n",
    "print('orig train set:',len(orig_train))\n",
    "\n",
    "new_test = set(new_test)\n",
    "data_d = pathlib.Path('./joints')\n",
    "new_train = list()\n",
    "for f in tqdm(list(sorted(data_d.iterdir())),desc='sample',ncols=150):\n",
    "    id = f.with_suffix('').name\n",
    "    if id not in new_test:\n",
    "        new_train.append(id)\n",
    "_ = open('HumanML3D/train.txt','w').write('\\n'.join(sorted(new_train)) + '\\n')\n",
    "print('new train set:',len(new_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the corresponding text file for each example in the new train/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q HumanML3D/texts.zip -d HumanML3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 26502/26502 [00:09<00:00, 2710.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import csv\n",
    "\n",
    "shutil.move('HumanML3D/texts','HumanML3D/texts_orig')\n",
    "\n",
    "texts_orig_d = pathlib.Path('HumanML3D/texts_orig')\n",
    "texts_d = pathlib.Path('HumanML3D/texts')\n",
    "texts_d.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "pose_data_to_joints_map_f = pathlib.Path('pose_data_to_joints_map.txt')\n",
    "new_id_to_orig_id = {row['new_id']:row['orig_id']\n",
    "                     for row in csv.DictReader(open(pose_data_to_joints_map_f))}\n",
    "for f in tqdm(list(sorted(data_d.iterdir())),desc='sample',ncols=150):\n",
    "    id = f.with_suffix('').name\n",
    "    text_f = texts_d / f'{id}.txt'\n",
    "    if id.startswith('M'):\n",
    "        id = id[1:]\n",
    "    orig_id = new_id_to_orig_id[id]\n",
    "    text_orig_f = texts_orig_d / f'{orig_id}.txt'\n",
    "    shutil.copyfile(text_orig_f,text_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shrik_mgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
