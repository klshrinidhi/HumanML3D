{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Poses from Amass Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please remember to download the following subdataset from AMASS website: https://amass.is.tue.mpg.de/download.php. Note only download the <u>SMPL+H G</u> data.\n",
    "* ACCD (ACCD)\n",
    "* HDM05 (MPI_HDM05)\n",
    "* TCDHands (TCD_handMocap)\n",
    "* SFU (SFU)\n",
    "* BMLmovi (BMLmovi)\n",
    "* CMU (CMU)\n",
    "* Mosh (MPI_mosh)\n",
    "* EKUT (EKUT)\n",
    "* KIT  (KIT)\n",
    "* Eyes_Janpan_Dataset (Eyes_Janpan_Dataset)\n",
    "* BMLhandball (BMLhandball)\n",
    "* Transitions (Transitions_mocap)\n",
    "* PosePrior (MPI_Limits)\n",
    "* HumanEva (HumanEva)\n",
    "* SSM (SSM_synced)\n",
    "* DFaust (DFaust_67)\n",
    "* TotalCapture (TotalCapture)\n",
    "* BMLrub (BioMotionLab_NTroje)\n",
    "\n",
    "### Unzip all datasets. In the bracket we give the name of the unzipped file folder. Please correct yours to the given names if they are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place all files under the directory **./amass_data/**. The directory structure shoud look like the following:  \n",
    "./amass_data/  \n",
    "./amass_data/ACCAD/  \n",
    "./amass_data/BioMotionLab_NTroje/  \n",
    "./amass_data/BMLhandball/  \n",
    "./amass_data/BMLmovi/   \n",
    "./amass_data/CMU/  \n",
    "./amass_data/DFaust_67/  \n",
    "./amass_data/EKUT/  \n",
    "./amass_data/Eyes_Japan_Dataset/  \n",
    "./amass_data/HumanEva/  \n",
    "./amass_data/KIT/  \n",
    "./amass_data/MPI_HDM05/  \n",
    "./amass_data/MPI_Limits/  \n",
    "./amass_data/MPI_mosh/  \n",
    "./amass_data/SFU/  \n",
    "./amass_data/SSM_synced/  \n",
    "./amass_data/TCD_handMocap/  \n",
    "./amass_data/TotalCapture/  \n",
    "./amass_data/Transitions_mocap/  \n",
    "\n",
    "**Please make sure the file path are correct, otherwise it can not succeed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "neutral_bm_path = 'body_models/smplx/SMPLX_NEUTRAL_2020.npz'\n",
    "num_betas = 16\n",
    "neutral_bm = BodyModel(bm_fname=neutral_bm_path, num_betas=num_betas, num_expressions=0).to(comp_device)\n",
    "faces = c2c(neutral_bm.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from collections import defaultdict\n",
    "from rich import print\n",
    "\n",
    "amass_data_d = pathlib.Path('./amass_data/')\n",
    "\n",
    "data_fs = [pathlib.Path(f'{rt}/{f}')\n",
    "           for rt,ds,fs in os.walk(amass_data_d,followlinks=True)\n",
    "           for f in fs\n",
    "           if f.endswith('.npz')]\n",
    "\n",
    "new_data_fs = list()\n",
    "files_not_readable = list()\n",
    "files_model_type_not_usable = list()\n",
    "files_mocap_frame_rate_key_missing = list()\n",
    "files_frame_rate_not_multiple = list()\n",
    "all_model_types = defaultdict(int)\n",
    "all_fps = defaultdict(int)\n",
    "for f in tqdm(data_fs,ncols=150):\n",
    "    try:\n",
    "        data = np.load(f,allow_pickle=True)\n",
    "    except zipfile.BadZipFile:\n",
    "        files_not_readable.append(f)\n",
    "        continue\n",
    "    all_model_types[data['surface_model_type'].item()] += 1\n",
    "    if data['surface_model_type'].item() not in {'smplx','smplx_locked_head'}:\n",
    "        files_model_type_not_usable.append(f)\n",
    "        continue\n",
    "    if 'mocap_frame_rate' not in data:\n",
    "        files_mocap_frame_rate_key_missing.append(f)\n",
    "        continue\n",
    "    fps = int(data['mocap_frame_rate'].item())\n",
    "    all_fps[fps] += 1\n",
    "    if fps % 30 != 0:\n",
    "        files_frame_rate_not_multiple.append(f)\n",
    "        continue\n",
    "    new_data_fs.append(f)\n",
    "print('total files:',len(data_fs))\n",
    "print('usable_files:',len(new_data_fs))\n",
    "print('files not readable:',len(files_not_readable))\n",
    "print('files model type not usable:',len(files_model_type_not_usable))\n",
    "print('files missing mocap_frame_rate:',len(files_mocap_frame_rate_key_missing))\n",
    "print('files with frame rante not multiple of 30fps:',len(files_frame_rate_not_multiple))\n",
    "print('model types:',*sorted(all_model_types.items()))\n",
    "print('mocap frame rate:',*sorted(all_fps.items()))\n",
    "assert len(data_fs) == len(new_data_fs) + len(files_not_readable) + len(files_model_type_not_usable) + len(files_mocap_frame_rate_key_missing) + len(files_frame_rate_not_multiple)\n",
    "data_fs = new_data_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_matrix = np.array([[1.0, 0.0, 0.0],\n",
    "                         [0.0, 0.0, 1.0],\n",
    "                         [0.0, 1.0, 0.0]])\n",
    "ex_fps = 30\n",
    "def amass_to_pose(src_path, save_path):\n",
    "    bdata = np.load(src_path, allow_pickle=True)\n",
    "    fps = bdata['mocap_frame_rate']\n",
    "    assert int(fps) % ex_fps == 0\n",
    "    assert bdata['surface_model_type'].item() == 'smplx'\n",
    "    assert bdata['gender'] == 'neutral'\n",
    "    bm = neutral_bm\n",
    "    down_sample = int(fps / ex_fps)\n",
    "    bdata_poses = bdata['poses'][::down_sample,...]\n",
    "    bdata_trans = bdata['trans'][::down_sample,...]\n",
    "    body_parms = {\n",
    "            'root_orient': torch.Tensor(bdata_poses[:, :3]).to(comp_device),\n",
    "            'pose_body': torch.Tensor(bdata_poses[:, 3:66]).to(comp_device),\n",
    "            'pose_hand': torch.Tensor(bdata_poses[:, 75:]).to(comp_device),\n",
    "            'trans': torch.Tensor(bdata_trans).to(comp_device),\n",
    "            'betas': torch.Tensor(np.repeat(bdata['betas'][:num_betas][np.newaxis], repeats=len(bdata_trans), axis=0)).to(comp_device),\n",
    "        }\n",
    "    with torch.no_grad():\n",
    "        body = bm(**body_parms)\n",
    "    pose_seq_np = body.Jtr.detach().cpu().numpy()\n",
    "    pose_seq_np_n = np.dot(pose_seq_np, trans_matrix)    \n",
    "    np.save(save_path, pose_seq_np_n)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_data_d = pathlib.Path('pose_data')\n",
    "for f in tqdm(data_fs,ncols=150):\n",
    "    out_f = pose_data_d / f.relative_to(amass_data_d).with_suffix('.npy')\n",
    "    if out_f.is_file():\n",
    "        out_f.unlink()\n",
    "    amass_to_pose(f,out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find pose_data/ -iname '*.npy' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment, Mirror and Relocate Motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "bm_params_f = pathlib.Path('./body_models/smplx/SMPLX_NEUTRAL_2020.npz')\n",
    "index_f = pathlib.Path('./index.csv')\n",
    "pose_data_d  = pathlib.Path('./pose_data')\n",
    "joints_d = pathlib.Path('./joints')\n",
    "fps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the corresponding left/right joints from model npy file. We will mirror left/right joints to augment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_params = np.load(bm_params_f,allow_pickle=True)\n",
    "joint2ind = bm_params['joint2num'].item()\n",
    "ind2joint = {v:k\n",
    "             for k,v in joint2ind.items()}\n",
    "l_joints,r_joints = list(),list()\n",
    "for j in joint2ind:\n",
    "    if j.startswith('L_'):\n",
    "        l_j = j\n",
    "        r_j = j.replace('L_','R_')\n",
    "        l_joints.append(joint2ind[l_j])\n",
    "        r_joints.append(joint2ind[r_j])\n",
    "# print('num joints to swap:',len(l_joints))\n",
    "# for l,r in sorted(zip(l_joints,r_joints)):\n",
    "#     print(ind2joint[l],ind2joint[r])\n",
    "joints_to_drop = [joint2ind['Jaw'],\n",
    "                  joint2ind['L_Eye'],\n",
    "                  joint2ind['R_Eye']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample frames according to HumanML3D, create the dictionary of files to sample, their start/end frames, their original ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sample = defaultdict(list)\n",
    "n_dropped = 0\n",
    "n_missing = 0\n",
    "for row in csv.DictReader(open(index_f)):\n",
    "    data_f = row['source_path']\n",
    "    # Not using `humanact12` dataset. Discard those entries in val set.\n",
    "    if 'humanact12' in data_f:\n",
    "        n_dropped += 1\n",
    "        continue\n",
    "    # SMPL-X version of AMASS is missing the following. Ignore them.\n",
    "    if ('BMLhandball' in data_f or\n",
    "        'DanceDB' in data_f or\n",
    "        'HUMAN4D' in data_f or \n",
    "        'CMU/22_23_Rory' in data_f or\n",
    "        'CMU/18_19_rory' in data_f or\n",
    "        'CMU/18_19_Justin' in data_f or\n",
    "        'CMU/20_21_rory1' in data_f or\n",
    "        'CMU/20_21_Justin1' in data_f or\n",
    "        'CMU/22_23_justin' in data_f):\n",
    "        n_dropped += 1\n",
    "        continue\n",
    "    # SMPL-X version of AMASS has renamed many files. Map the old names to new.\n",
    "    data_f = (data_f\n",
    "              .replace('/BioMotionLab_NTroje/','/BMLrub/')\n",
    "              .replace('/DFaust_67/','/DFaust/')\n",
    "              .replace('/MPI_mosh/','/MoSh/')\n",
    "              .replace('/MPI_HDM05/','/HDM05/')\n",
    "              .replace('/MPI_Limits/','/PosePrior/')\n",
    "              .replace('/SSM_synced/','/SSM/')\n",
    "              .replace('/TCD_handMocap/','/TCDHands/')\n",
    "              .replace('/Transitions_mocap/','/Transitions/')\n",
    "              .replace('.npz','')\n",
    "              .replace('.npy','')\n",
    "              .replace('_poses','')\n",
    "              .replace(' ','_'))\n",
    "    data_f = pathlib.Path(data_f)\n",
    "    data_d = data_f.parent\n",
    "    assert data_d.is_dir(),data_d\n",
    "    for f in data_d.iterdir():\n",
    "        if f.name.startswith(data_f.name):\n",
    "            to_sample[f].append((int(row['start_frame']),\n",
    "                                 int(row['end_frame']),\n",
    "                                 row['new_name'].replace('.npy','')))\n",
    "            break\n",
    "    else:\n",
    "        # assert False\n",
    "        n_missing += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many files need to be sampled multiple times because there are multiple entries in `index.csv` for those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('samples dropped:',n_dropped)\n",
    "print('samples missing:',n_missing)\n",
    "print('files to sample:',len(to_sample))\n",
    "print('files with >1 sample:', \n",
    "      sum(1\n",
    "          for v in to_sample.values()\n",
    "          if len(v) > 1))\n",
    "print('total samples:',\n",
    "      sum(len(v)\n",
    "          for v in to_sample.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fs = [pathlib.Path(f'{rt}/{f}')\n",
    "           for rt,ds,fs in os.walk(pose_data_d,followlinks=True)\n",
    "           for f in fs\n",
    "           if f.endswith('.npy')]\n",
    "data_fs.sort()\n",
    "print('num files:',len(data_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample each file as per HumanML3D, create a mirrored version of the sample, save both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dropped = 0\n",
    "pose_data_to_joints_map = ['file,new_id,orig_id']\n",
    "pbar = tqdm(data_fs,desc='mirroring/pruning files',ncols=150)\n",
    "for i,f in enumerate(pbar):\n",
    "    if f not in to_sample:\n",
    "        n_dropped += 1\n",
    "        continue\n",
    "    for j,(i_beg,i_end,orig_id) in enumerate(sorted(to_sample[f])):\n",
    "        id = f'{i:06}_{j:02}'\n",
    "        id_m = f'M{i:06}_{j:02}'\n",
    "        out_f = joints_d / f'{id}.npy'\n",
    "        out_m_f = joints_d / f'{id_m}.npy'\n",
    "        data = np.load(f)\n",
    "        if 'humanact12' not in str(f):\n",
    "            if 'Eyes_Japan_Dataset' in str(f):\n",
    "                data = data[3*fps:]\n",
    "            if 'HDM05' in str(f):\n",
    "                data = data[3*fps:]\n",
    "            if 'TotalCapture' in str(f):\n",
    "                data = data[1*fps:]\n",
    "            if 'PosePrior' in str(f):\n",
    "                data = data[1*fps:]\n",
    "            if 'Transitions' in str(f):\n",
    "                data = data[int(0.5*fps):]\n",
    "        data = data[i_beg:i_end]\n",
    "        if out_f.is_file() and out_m_f.is_file():\n",
    "            pose_data_to_joints_map.append(f'{f},{id},{orig_id}')    \n",
    "            continue\n",
    "        data[...,0] *= -1\n",
    "        data_m = data.copy()\n",
    "        data_m[:,l_joints] = data[:,r_joints]\n",
    "        data_m[:,r_joints] = data[:,l_joints]\n",
    "        data = np.delete(data,joints_to_drop,axis=1)\n",
    "        data_m = np.delete(data_m,joints_to_drop,axis=1)\n",
    "        np.save(out_f,data)\n",
    "        np.save(out_m_f,data_m)\n",
    "        pose_data_to_joints_map.append(f'{f},{id},{orig_id}')\n",
    "        pbar.set_postfix({'samples':2*len(pose_data_to_joints_map),\n",
    "                          'dropped':n_dropped})\n",
    "_ = open('pose_data_to_joints_map.txt','w').write('\\n'.join(pose_data_to_joints_map) + '\\n')\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout original train/val/test sets and texts. Just to make sure we have the original versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout main -- HumanML3D/train.txt HumanML3D/val.txt HumanML3D/test.txt HumanML3D/texts.zip\n",
    "!rm -rf HumanML3D/train_orig.txt HumanML3D/val_orig.txt HumanML3D/test_orig.txt HumanML3D/texts_orig HumanML3D/texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "_ = shutil.move('HumanML3D/train.txt','HumanML3D/train_orig.txt')\n",
    "_ = shutil.move('HumanML3D/val.txt','HumanML3D/val_orig.txt')\n",
    "_ = shutil.move('HumanML3D/test.txt','HumanML3D/test_orig.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new test set from original test set with missing samples removed. SMPL-X version of AMASS doesn't seem to have all the original samples from SMPL-H version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test = [l.strip()\n",
    "             for l in open('./HumanML3D/test_orig.txt')]\n",
    "print('orig test set:',len(orig_test))\n",
    "\n",
    "pose_data_to_joints_map_f = pathlib.Path('pose_data_to_joints_map.txt')\n",
    "orig_id_to_new_id = {row['orig_id']:row['new_id']\n",
    "                     for row in csv.DictReader(open(pose_data_to_joints_map_f))}\n",
    "\n",
    "index_f = pathlib.Path('./index.csv')\n",
    "orig_id_to_orig_file = {row['new_name'].replace('.npy',''):row['source_path']\n",
    "                        for row in csv.DictReader(open(index_f))}\n",
    "\n",
    "new_test = list()\n",
    "for orig_id in orig_test:\n",
    "    is_m_id = orig_id.startswith('M')\n",
    "    if is_m_id:\n",
    "        orig_m_id = orig_id\n",
    "        orig_id = orig_m_id[1:]\n",
    "    else:\n",
    "        orig_m_id = f'M{orig_id}'\n",
    "    data_f = orig_id_to_orig_file[orig_id]\n",
    "    # Not using `humanact12` dataset. Discard those entries in val set.\n",
    "    if 'humanact12' in data_f:\n",
    "        continue\n",
    "    # SMPL-X version of AMASS is missing the following. Ignore them.\n",
    "    if ('BMLhandball' in data_f or\n",
    "        'DanceDB' in data_f or\n",
    "        'HUMAN4D' in data_f or \n",
    "        'CMU/22_23_Rory' in data_f or\n",
    "        'CMU/18_19_rory' in data_f or\n",
    "        'CMU/18_19_Justin' in data_f or\n",
    "        'CMU/20_21_rory1' in data_f or\n",
    "        'CMU/20_21_Justin1' in data_f or\n",
    "        'CMU/22_23_justin' in data_f):\n",
    "        continue\n",
    "    if orig_id not in orig_id_to_new_id:\n",
    "        continue\n",
    "    new_test.append(orig_id_to_new_id[orig_id])\n",
    "open('HumanML3D/test.txt','w').write('\\n'.join(sorted(new_test))+'\\n')\n",
    "print('new test set:',len(new_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the remaining samples as the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train = [l.strip()\n",
    "             for l in open('./HumanML3D/train_orig.txt')]\n",
    "print('orig train set:',len(orig_train))\n",
    "\n",
    "new_test = set(new_test)\n",
    "data_d = pathlib.Path('./joints')\n",
    "new_train = list()\n",
    "for f in tqdm(list(sorted(data_d.iterdir())),desc='sample',ncols=150):\n",
    "    id = f.with_suffix('').name\n",
    "    if id not in new_test:\n",
    "        new_train.append(id)\n",
    "_ = open('HumanML3D/train.txt','w').write('\\n'.join(sorted(new_train)) + '\\n')\n",
    "print('new train set:',len(new_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the corresponding text file for each example in the new train/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q HumanML3D/texts.zip -d HumanML3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import csv\n",
    "\n",
    "shutil.move('HumanML3D/texts','HumanML3D/texts_orig')\n",
    "\n",
    "texts_orig_d = pathlib.Path('HumanML3D/texts_orig')\n",
    "texts_d = pathlib.Path('HumanML3D/texts')\n",
    "texts_d.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "pose_data_to_joints_map_f = pathlib.Path('pose_data_to_joints_map.txt')\n",
    "new_id_to_orig_id = {row['new_id']:row['orig_id']\n",
    "                     for row in csv.DictReader(open(pose_data_to_joints_map_f))}\n",
    "for f in tqdm(list(sorted(data_d.iterdir())),desc='sample',ncols=150):\n",
    "    id = f.with_suffix('').name\n",
    "    text_f = texts_d / f'{id}.txt'\n",
    "    if id.startswith('M'):\n",
    "        id = id[1:]\n",
    "    orig_id = new_id_to_orig_id[id]\n",
    "    text_orig_f = texts_orig_d / f'{orig_id}.txt'\n",
    "    shutil.copyfile(text_orig_f,text_f)\n",
    "\n",
    "shutil.rmtree('HumanML3D/texts_orig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75090/75090 [04:51<00:00, 257.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import pathlib\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "texts_1_d = pathlib.Path('/vision/changan/shrinidhi/data2/HumanML3D_amass30_smplx_2020/texts')\n",
    "texts_2_d = pathlib.Path('/vision/changan/shrinidhi/data2/HumanML3D_beat2_smplx_2020/texts')\n",
    "out_d = pathlib.Path('/vision/changan/shrinidhi/data2/HumanML3D_amass30_beat2_smplx_2020/texts')\n",
    "\n",
    "for f in tqdm(list(chain(texts_1_d.iterdir(),texts_2_d.iterdir())),ncols=150):\n",
    "    shutil.copyfile(f, out_d / f.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shrik_mgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
