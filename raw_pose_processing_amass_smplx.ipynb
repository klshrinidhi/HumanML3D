{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Poses from Amass Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please remember to download the following subdataset from AMASS website: https://amass.is.tue.mpg.de/download.php. Note only download the <u>SMPL+H G</u> data.\n",
    "* ACCD (ACCD)\n",
    "* HDM05 (MPI_HDM05)\n",
    "* TCDHands (TCD_handMocap)\n",
    "* SFU (SFU)\n",
    "* BMLmovi (BMLmovi)\n",
    "* CMU (CMU)\n",
    "* Mosh (MPI_mosh)\n",
    "* EKUT (EKUT)\n",
    "* KIT  (KIT)\n",
    "* Eyes_Janpan_Dataset (Eyes_Janpan_Dataset)\n",
    "* BMLhandball (BMLhandball)\n",
    "* Transitions (Transitions_mocap)\n",
    "* PosePrior (MPI_Limits)\n",
    "* HumanEva (HumanEva)\n",
    "* SSM (SSM_synced)\n",
    "* DFaust (DFaust_67)\n",
    "* TotalCapture (TotalCapture)\n",
    "* BMLrub (BioMotionLab_NTroje)\n",
    "\n",
    "### Unzip all datasets. In the bracket we give the name of the unzipped file folder. Please correct yours to the given names if they are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place all files under the directory **./amass_data/**. The directory structure shoud look like the following:  \n",
    "./amass_data/  \n",
    "./amass_data/ACCAD/  \n",
    "./amass_data/BioMotionLab_NTroje/  \n",
    "./amass_data/BMLhandball/  \n",
    "./amass_data/BMLmovi/   \n",
    "./amass_data/CMU/  \n",
    "./amass_data/DFaust_67/  \n",
    "./amass_data/EKUT/  \n",
    "./amass_data/Eyes_Japan_Dataset/  \n",
    "./amass_data/HumanEva/  \n",
    "./amass_data/KIT/  \n",
    "./amass_data/MPI_HDM05/  \n",
    "./amass_data/MPI_Limits/  \n",
    "./amass_data/MPI_mosh/  \n",
    "./amass_data/SFU/  \n",
    "./amass_data/SSM_synced/  \n",
    "./amass_data/TCD_handMocap/  \n",
    "./amass_data/TotalCapture/  \n",
    "./amass_data/Transitions_mocap/  \n",
    "\n",
    "**Please make sure the file path are correct, otherwise it can not succeed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "neutral_bm_path = 'body_models/smplx/SMPLX_NEUTRAL_2020.npz'\n",
    "num_betas = 16\n",
    "neutral_bm = BodyModel(bm_fname=neutral_bm_path, num_betas=num_betas, num_expressions=0).to(comp_device)\n",
    "faces = c2c(neutral_bm.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 17430/17430 [00:28<00:00, 618.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files: 17430\n",
      "usable_files: 9977\n",
      "files not readable: 1\n",
      "files model type not usable: 0\n",
      "files missing mocap_frame_rate: 462\n",
      "files with frame rante not multiple of 30fps: 6990\n",
      "model types: ('smplx', 17429)\n",
      "mocap frame rate: (59, 13) (60, 532) (100, 6920) (120, 9436) (150, 9) (250, 57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from collections import defaultdict\n",
    "\n",
    "amass_data_d = pathlib.Path('amass_data')\n",
    "\n",
    "data_fs = [pathlib.Path(f'{rt}/{f}')\n",
    "           for rt,ds,fs in os.walk(amass_data_d)\n",
    "           for f in fs\n",
    "           if f.endswith('.npz')]\n",
    "data_fs.sort()\n",
    "\n",
    "new_data_fs = list()\n",
    "files_not_readable = list()\n",
    "files_model_type_not_usable = list()\n",
    "files_mocap_frame_rate_key_missing = list()\n",
    "files_frame_rate_not_multiple_of_30 = list()\n",
    "all_model_types = defaultdict(int)\n",
    "all_fps = defaultdict(int)\n",
    "for f in tqdm(data_fs,ncols=150):\n",
    "    try:\n",
    "        data = np.load(f,allow_pickle=True)\n",
    "    except zipfile.BadZipFile:\n",
    "        files_not_readable.append(f)\n",
    "        continue\n",
    "    all_model_types[data['surface_model_type'].item()] += 1\n",
    "    if data['surface_model_type'].item() not in {'smplx','smplx_locked_head'}:\n",
    "        files_model_type_not_usable.append(f)\n",
    "        continue\n",
    "    if 'mocap_frame_rate' not in data:\n",
    "        files_mocap_frame_rate_key_missing.append(f)\n",
    "        continue\n",
    "    fps = int(data['mocap_frame_rate'].item())\n",
    "    all_fps[fps] += 1\n",
    "    if fps % 30 != 0:\n",
    "        files_frame_rate_not_multiple_of_30.append(f)\n",
    "        continue\n",
    "    new_data_fs.append(f)\n",
    "print('total files:',len(data_fs))\n",
    "print('usable_files:',len(new_data_fs))\n",
    "print('files not readable:',len(files_not_readable))\n",
    "print('files model type not usable:',len(files_model_type_not_usable))\n",
    "print('files missing mocap_frame_rate:',len(files_mocap_frame_rate_key_missing))\n",
    "print('files with frame rante not multiple of 30fps:',len(files_frame_rate_not_multiple_of_30))\n",
    "print('model types:',*sorted(all_model_types.items()))\n",
    "print('mocap frame rate:',*sorted(all_fps.items()))\n",
    "assert len(data_fs) == (len(new_data_fs) + \n",
    "                        len(files_not_readable) + \n",
    "                        len(files_model_type_not_usable) + \n",
    "                        len(files_mocap_frame_rate_key_missing) + \n",
    "                        len(files_frame_rate_not_multiple_of_30))\n",
    "data_fs = new_data_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_matrix = np.array([[1.0, 0.0, 0.0],\n",
    "                         [0.0, 0.0, 1.0],\n",
    "                         [0.0, 1.0, 0.0]])\n",
    "ex_fps = 30\n",
    "vis = 4\n",
    "def amass_to_pose(motion_f, joints_f):\n",
    "    bdata = np.load(motion_f, allow_pickle=True)\n",
    "    fps = bdata['mocap_frame_rate']\n",
    "    assert int(fps) % ex_fps == 0\n",
    "    assert bdata['surface_model_type'].item() == 'smplx'\n",
    "    assert bdata['gender'] == 'neutral'\n",
    "    bm = neutral_bm\n",
    "    down_sample = int(fps / ex_fps)\n",
    "    bdata_poses = bdata['poses'][::down_sample,...]\n",
    "    bdata_trans = bdata['trans'][::down_sample,...]\n",
    "    body_parms = {\n",
    "            'root_orient': torch.Tensor(bdata_poses[:, :3]).to(comp_device),\n",
    "            'pose_body': torch.Tensor(bdata_poses[:, 3:66]).to(comp_device),\n",
    "            'pose_hand': torch.Tensor(bdata_poses[:, 75:]).to(comp_device),\n",
    "            'trans': torch.Tensor(bdata_trans).to(comp_device),\n",
    "            'betas': torch.Tensor(np.repeat(bdata['betas'][:num_betas][np.newaxis], repeats=len(bdata_trans), axis=0)).to(comp_device),\n",
    "        }\n",
    "    with torch.no_grad():\n",
    "        body = bm(**body_parms)\n",
    "    pose_seq_np = body.Jtr.detach().cpu().numpy()\n",
    "    # Make XZ plane the ground plane.\n",
    "    pose_seq_np_n = np.dot(pose_seq_np, trans_matrix)    \n",
    "    np.save(joints_f,pose_seq_np_n)\n",
    "\n",
    "    ############################################################################\n",
    "    global vis\n",
    "    if vis > 0:\n",
    "        import trimesh\n",
    "        vertices_all = body.v.detach().cpu().numpy()\n",
    "        faces = body.f.detach().cpu().numpy()\n",
    "        src_f = pathlib.Path(motion_f).relative_to('amass_data')\n",
    "        ply_d = (pathlib.Path('/vision/vision_data_2/VGGSound_shards_fixed/shrinidhi/meshes_amass_pose_data') / \n",
    "                src_f.parent / src_f.stem)\n",
    "        ply_d.mkdir(parents=True,exist_ok=True)\n",
    "        for f,vertices in enumerate(vertices_all):\n",
    "            obj_f = ply_d / f'{f:05}.ply'\n",
    "            mesh = trimesh.Trimesh(vertices=vertices,\n",
    "                                faces=faces,\n",
    "                                process=False)\n",
    "            mesh.export(obj_f)\n",
    "        vis -= 1\n",
    "    ############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amass-to-pose:   0%|                                                                                                         | 0/9977 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amass-to-pose: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 9977/9977 [05:40<00:00, 29.32it/s]\n"
     ]
    }
   ],
   "source": [
    "pose_data_d = pathlib.Path('amass_pose_data')\n",
    "for f in tqdm(data_fs,desc='amass-to-pose',ncols=150):\n",
    "    out_f = pose_data_d / f.relative_to(amass_data_d).with_suffix('.npy')\n",
    "    out_f.parent.mkdir(parents=True,exist_ok=True)\n",
    "    if out_f.is_file():\n",
    "        out_f.unlink()\n",
    "    amass_to_pose(f,out_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment, Mirror and Relocate Motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "bm_params_f = pathlib.Path('body_models/smplx/SMPLX_NEUTRAL_2020.npz')\n",
    "index_f = pathlib.Path('index.csv')\n",
    "pose_data_d  = pathlib.Path('pose_data_amass')\n",
    "joints_d = pathlib.Path('joints_amass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the corresponding left/right joints from model npy file. We will mirror left/right joints to augment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num joints to swap: 24\n",
      "left joints: [30, 23, 13, 34, 35, 36, 39, 38, 37, 27, 26, 25, 10, 4, 18, 31, 32, 33, 28, 29, 20, 16, 1, 7]\n",
      "right joints: [45, 24, 14, 49, 50, 51, 54, 53, 52, 42, 41, 40, 11, 5, 19, 46, 47, 48, 43, 44, 21, 17, 2, 8]\n",
      "joints to drop: [22, 23, 24]\n",
      "L_Hip      ( 1) <--> ( 2) R_Hip\n",
      "L_Knee     ( 4) <--> ( 5) R_Knee\n",
      "L_Ankle    ( 7) <--> ( 8) R_Ankle\n",
      "L_Foot     (10) <--> (11) R_Foot\n",
      "L_Collar   (13) <--> (14) R_Collar\n",
      "L_Shoulder (16) <--> (17) R_Shoulder\n",
      "L_Elbow    (18) <--> (19) R_Elbow\n",
      "L_Wrist    (20) <--> (21) R_Wrist\n",
      "L_Eye      (23) <--> (24) R_Eye\n",
      "L_Index1   (25) <--> (40) R_Index1\n",
      "L_Index2   (26) <--> (41) R_Index2\n",
      "L_Index3   (27) <--> (42) R_Index3\n",
      "L_Middle1  (28) <--> (43) R_Middle1\n",
      "L_Middle2  (29) <--> (44) R_Middle2\n",
      "L_Middle3  (30) <--> (45) R_Middle3\n",
      "L_Pinky1   (31) <--> (46) R_Pinky1\n",
      "L_Pinky2   (32) <--> (47) R_Pinky2\n",
      "L_Pinky3   (33) <--> (48) R_Pinky3\n",
      "L_Ring1    (34) <--> (49) R_Ring1\n",
      "L_Ring2    (35) <--> (50) R_Ring2\n",
      "L_Ring3    (36) <--> (51) R_Ring3\n",
      "L_Thumb1   (37) <--> (52) R_Thumb1\n",
      "L_Thumb2   (38) <--> (53) R_Thumb2\n",
      "L_Thumb3   (39) <--> (54) R_Thumb3\n"
     ]
    }
   ],
   "source": [
    "bm_params = np.load(bm_params_f,allow_pickle=True)\n",
    "joint2ind = bm_params['joint2num'].item()\n",
    "ind2joint = {v:k\n",
    "             for k,v in joint2ind.items()}\n",
    "l_joints,r_joints = list(),list()\n",
    "for j in joint2ind:\n",
    "    if j.startswith('L_'):\n",
    "        l_j = j\n",
    "        r_j = j.replace('L_','R_')\n",
    "        l_joints.append(joint2ind[l_j])\n",
    "        r_joints.append(joint2ind[r_j])\n",
    "joints_to_drop = [joint2ind['Jaw'],\n",
    "                  joint2ind['L_Eye'],\n",
    "                  joint2ind['R_Eye']]\n",
    "\n",
    "print('num joints to swap:',len(l_joints))\n",
    "print('left joints:',l_joints)\n",
    "print('right joints:',r_joints)\n",
    "print('joints to drop:',joints_to_drop)\n",
    "for l,r in sorted(zip(l_joints,r_joints)):\n",
    "    print(f'{ind2joint[l]:10} ({l:2}) <--> ({r:2}) {ind2joint[r]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample frames according to HumanML3D, create the dictionary of files to sample, their start/end frames, their original ids.  \n",
    "Some sub-datasets of AMASS are not used by HumanML3D because these sub-datasets were added AMASS after HumanML3D was released. Refer to [this issue](https://github.com/EricGuo5513/HumanML3D/issues/96)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing files: 5192\n"
     ]
    }
   ],
   "source": [
    "to_sample = defaultdict(list)\n",
    "n_dropped = 0\n",
    "n_missing = 0\n",
    "for row in csv.DictReader(open(index_f)):\n",
    "    data_f = row['source_path']\n",
    "    data_f = data_f.replace('pose_data','pose_data_amass')\n",
    "    # Not using `humanact12` dataset. Discard those entries.\n",
    "    if 'humanact12' in data_f:\n",
    "        n_dropped += 1\n",
    "        continue\n",
    "    # SMPL-X version of AMASS is missing the following. Discard those entries.\n",
    "    if ('BMLhandball' in data_f or\n",
    "        'DanceDB' in data_f or\n",
    "        'HUMAN4D' in data_f or \n",
    "        'CMU/22_23_Rory' in data_f or\n",
    "        'CMU/18_19_rory' in data_f or\n",
    "        'CMU/18_19_Justin' in data_f or\n",
    "        'CMU/20_21_rory1' in data_f or\n",
    "        'CMU/20_21_Justin1' in data_f or\n",
    "        'CMU/22_23_justin' in data_f):\n",
    "        n_dropped += 1\n",
    "        continue\n",
    "    # SMPL-X version of AMASS has renamed many files. Map the old names to new.\n",
    "    data_f = (data_f\n",
    "              .replace('/BioMotionLab_NTroje/','/BMLrub/')\n",
    "              .replace('/DFaust_67/','/DFaust/')\n",
    "              .replace('/MPI_mosh/','/MoSh/')\n",
    "              .replace('/MPI_HDM05/','/HDM05/')\n",
    "              .replace('/MPI_Limits/','/PosePrior/')\n",
    "              .replace('/SSM_synced/','/SSM/')\n",
    "              .replace('/TCD_handMocap/','/TCDHands/')\n",
    "              .replace('/Transitions_mocap/','/Transitions/')\n",
    "              .replace('.npz','')\n",
    "              .replace('.npy','')\n",
    "              .replace('_poses','')\n",
    "              .replace(' ','_'))\n",
    "    data_f = pathlib.Path(data_f)\n",
    "    data_d = data_f.parent\n",
    "    if not data_d.is_dir():\n",
    "        n_missing += 1\n",
    "        continue\n",
    "    for f in data_d.iterdir():\n",
    "        if f.name.startswith(data_f.name):\n",
    "            to_sample[f].append((int(row['start_frame']),\n",
    "                                 int(row['end_frame']),\n",
    "                                 row['new_name'].replace('.npy','')))\n",
    "            break\n",
    "    else:\n",
    "        # assert False\n",
    "        n_missing += 1\n",
    "print('missing files:',n_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many files need to be sampled multiple times because there are multiple entries in `index.csv` for those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dropped: 1365\n",
      "samples missing: 5192\n",
      "files to sample: 5498\n",
      "files with >1 sample: 2067\n",
      "total samples: 8059\n"
     ]
    }
   ],
   "source": [
    "print('samples dropped:',n_dropped)\n",
    "print('samples missing:',n_missing)\n",
    "print('files to sample:',len(to_sample))\n",
    "print('files with >1 sample:', \n",
    "      sum(1\n",
    "          for v in to_sample.values()\n",
    "          if len(v) > 1))\n",
    "print('total samples:',\n",
    "      sum(len(v)\n",
    "          for v in to_sample.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num files: 9977\n"
     ]
    }
   ],
   "source": [
    "data_fs = [pathlib.Path(f'{rt}/{f}')\n",
    "           for rt,ds,fs in os.walk(pose_data_d)\n",
    "           for f in fs\n",
    "           if f.endswith('.npy')]\n",
    "data_fs.sort()\n",
    "print('num files:',len(data_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample each file as per HumanML3D, create a mirrored version of the sample, save both.  \n",
    "Following code is explained in [this issue](https://github.com/EricGuo5513/HumanML3D/issues/20).\n",
    "```\n",
    "data[...,0] *= -1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mirror & prune files: 100%|██████████████████████████████████████████████████████████| 9977/9977 [02:07<00:00, 78.02it/s, samples=16120, dropped=4478]\n"
     ]
    }
   ],
   "source": [
    "n_dropped = 0\n",
    "pose_data_to_joints_map = ['file,new_id,orig_id']\n",
    "pbar = tqdm(data_fs,desc='mirror & prune files',ncols=150)\n",
    "for i,f in enumerate(pbar):\n",
    "    if f not in to_sample:\n",
    "        n_dropped += 1\n",
    "        continue\n",
    "    for j,(i_beg,i_end,orig_id) in enumerate(sorted(to_sample[f])):\n",
    "        id = f'{i:06}_{j:02}'\n",
    "        id_m = f'M{i:06}_{j:02}'\n",
    "        out_f = joints_d / f'{id}.npy'\n",
    "        out_m_f = joints_d / f'{id_m}.npy'\n",
    "        if out_f.is_file():\n",
    "            out_f.unlink()\n",
    "        if out_m_f.is_file():\n",
    "            out_m_f.unlink()\n",
    "        data = np.load(f)\n",
    "        if 'humanact12' not in str(f):\n",
    "            if 'Eyes_Japan_Dataset' in str(f):\n",
    "                data = data[3*fps:]\n",
    "            if 'HDM05' in str(f):\n",
    "                data = data[3*fps:]\n",
    "            if 'TotalCapture' in str(f):\n",
    "                data = data[1*fps:]\n",
    "            if 'PosePrior' in str(f):\n",
    "                data = data[1*fps:]\n",
    "            if 'Transitions' in str(f):\n",
    "                data = data[int(0.5*fps):]\n",
    "        data = data[i_beg:i_end]\n",
    "        data_m = data.copy()\n",
    "        data_m[:,l_joints] = data[:,r_joints]\n",
    "        data_m[:,r_joints] = data[:,l_joints]\n",
    "        data[...,0] *= -1\n",
    "        data = np.delete(data,joints_to_drop,axis=1)\n",
    "        data_m = np.delete(data_m,joints_to_drop,axis=1)\n",
    "        np.save(out_f,data)\n",
    "        np.save(out_m_f,data_m)\n",
    "        pose_data_to_joints_map.append(f'{f},{id},{orig_id}')\n",
    "        pbar.set_postfix({'samples':2*len(pose_data_to_joints_map),\n",
    "                          'dropped':n_dropped})\n",
    "_ = open('pose_data_to_joints_map_amass.txt','w').write('\\n'.join(pose_data_to_joints_map) + '\\n')\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout original train/val/test sets and texts. Just to make sure we have the original versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git checkout main -- HumanML3D/train.txt HumanML3D/val.txt HumanML3D/test.txt HumanML3D/texts.zip\n",
    "!rm -rf HumanML3D/train_orig.txt HumanML3D/val_orig.txt HumanML3D/test_orig.txt HumanML3D/texts_orig HumanML3D/texts\n",
    "!unzip -q HumanML3D/texts.zip -d HumanML3D\n",
    "!rm HumanML3D/texts.zip\n",
    "!mv HumanML3D/train.txt HumanML3D_amass/train_orig.txt\n",
    "!mv HumanML3D/val.txt HumanML3D_amass/val_orig.txt\n",
    "!mv HumanML3D/test.txt HumanML3D_amass/test_orig.txt\n",
    "!mv HumanML3D/texts HumanML3D_amass/texts_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new test set from original test set with missing samples removed. SMPL-X version of AMASS doesn't seem to have all the original samples from SMPL-H version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig test set: 4384\n",
      "new test set: 2460\n"
     ]
    }
   ],
   "source": [
    "orig_test = [l.strip()\n",
    "             for l in open('HumanML3D_amass/test_orig.txt')]\n",
    "print('orig test set:',len(orig_test))\n",
    "\n",
    "pose_data_to_joints_map_f = pathlib.Path('pose_data_to_joints_map_amass.txt')\n",
    "orig_id_to_new_id = {row['orig_id']:row['new_id']\n",
    "                     for row in csv.DictReader(open(pose_data_to_joints_map_f))}\n",
    "\n",
    "index_f = pathlib.Path('index.csv')\n",
    "orig_id_to_orig_file = {row['new_name'].replace('.npy',''):row['source_path']\n",
    "                        for row in csv.DictReader(open(index_f))}\n",
    "\n",
    "new_test = list()\n",
    "for orig_id in orig_test:\n",
    "    is_m_id = orig_id.startswith('M')\n",
    "    if is_m_id:\n",
    "        orig_m_id = orig_id\n",
    "        orig_id = orig_m_id[1:]\n",
    "    else:\n",
    "        orig_m_id = f'M{orig_id}'\n",
    "    data_f = orig_id_to_orig_file[orig_id]\n",
    "    # Not using `humanact12` dataset. Discard those entries in val set.\n",
    "    if 'humanact12' in data_f:\n",
    "        continue\n",
    "    # SMPL-X version of AMASS is missing the following. Ignore them.\n",
    "    if ('BMLhandball' in data_f or\n",
    "        'DanceDB' in data_f or\n",
    "        'HUMAN4D' in data_f or \n",
    "        'CMU/22_23_Rory' in data_f or\n",
    "        'CMU/18_19_rory' in data_f or\n",
    "        'CMU/18_19_Justin' in data_f or\n",
    "        'CMU/20_21_rory1' in data_f or\n",
    "        'CMU/20_21_Justin1' in data_f or\n",
    "        'CMU/22_23_justin' in data_f):\n",
    "        continue\n",
    "    if orig_id not in orig_id_to_new_id:\n",
    "        continue\n",
    "    new_test.append(orig_id_to_new_id[orig_id])\n",
    "open('HumanML3D_amass/test.txt','w').write('\\n'.join(sorted(new_test))+'\\n')\n",
    "print('new test set:',len(new_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the remaining samples as the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig train set: 23384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 16118/16118 [00:00<00:00, 375747.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new train set: 14888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "orig_train = [l.strip()\n",
    "             for l in open('./HumanML3D_amass/train_orig.txt')]\n",
    "print('orig train set:',len(orig_train))\n",
    "\n",
    "new_test = set(new_test)\n",
    "joints_d = pathlib.Path('joints_amass')\n",
    "new_train = list()\n",
    "for f in tqdm(list(sorted(joints_d.iterdir())),desc='sample',ncols=150):\n",
    "    id = f.with_suffix('').name\n",
    "    if id not in new_test:\n",
    "        new_train.append(id)\n",
    "_ = open('HumanML3D_amass/train.txt','w').write('\\n'.join(sorted(new_train)) + '\\n')\n",
    "print('new train set:',len(new_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm HumanML3D_amass/train_orig.txt HumanML3D_amass/val_orig.txt HumanML3D_amass/test_orig.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the corresponding text file for each example in the new train/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "texts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 16118/16118 [02:30<00:00, 106.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import csv\n",
    "\n",
    "texts_orig_d = pathlib.Path('HumanML3D_amass/texts_orig')\n",
    "texts_d = pathlib.Path('HumanML3D_amass/texts')\n",
    "texts_d.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "pose_data_to_joints_map_f = pathlib.Path('pose_data_to_joints_map_amass.txt')\n",
    "new_id_to_orig_id = {row['new_id']:row['orig_id']\n",
    "                     for row in csv.DictReader(open(pose_data_to_joints_map_f))}\n",
    "for f in tqdm(list(sorted(joints_d.iterdir())),desc='texts',ncols=150):\n",
    "    id = f.with_suffix('').name\n",
    "    text_f = texts_d / f'{id}.txt'\n",
    "    if id.startswith('M'):\n",
    "        id = id[1:]\n",
    "    orig_id = new_id_to_orig_id[id]\n",
    "    text_orig_f = texts_orig_d / f'{orig_id}.txt'\n",
    "    shutil.copyfile(text_orig_f,text_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: avjoint\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -r HumanML3D_amass/texts_orig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shrik_mgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
