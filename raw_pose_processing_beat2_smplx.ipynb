{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Poses from Amass Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "neutral_bm_path = './body_models/smplx/SMPLX_NEUTRAL_2020.npz'\n",
    "neutral_bm = BodyModel(bm_fname=neutral_bm_path, num_betas=300, num_expressions=100).to(comp_device)\n",
    "faces = c2c(neutral_bm.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num files: 1620\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "for root, dirs, files in os.walk('./beat2_data'):\n",
    "    for name in files:\n",
    "        if name.endswith('.npz'):\n",
    "            paths.append(os.path.join(root, name))\n",
    "print('num files:',len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amass_to_pose(src_path, save_path):\n",
    "    bdata = np.load(src_path, allow_pickle=True)\n",
    "    assert bdata['mocap_frame_rate'].item() == 30\n",
    "    assert bdata['model'].item() == 'smplx2020'\n",
    "    assert bdata['gender'].item() == 'neutral'\n",
    "    bm = neutral_bm\n",
    "    T = bdata['poses'].shape[0]\n",
    "    B = 1024 # batch size\n",
    "    results = list()\n",
    "    for i_beg in range(0,T,B):\n",
    "        bdata_poses = bdata['poses'][i_beg:i_beg+B]\n",
    "        bdata_trans = bdata['trans'][i_beg:i_beg+B]\n",
    "        bdata_expression = bdata['expressions'][i_beg:i_beg+B]\n",
    "        body_parms = {\n",
    "                'root_orient': torch.Tensor(bdata_poses[:,:3]).to(comp_device),\n",
    "                'pose_body'  : torch.Tensor(bdata_poses[:,3:66]).to(comp_device),\n",
    "                'pose_hand'  : torch.Tensor(bdata_poses[:,75:]).to(comp_device),\n",
    "                'trans'      : torch.Tensor(bdata_trans).to(comp_device),\n",
    "                'betas'      : torch.Tensor(np.repeat(bdata['betas'][np.newaxis], repeats=len(bdata_trans), axis=0)).to(comp_device),\n",
    "                'expression' : torch.Tensor(bdata_expression).to(comp_device)\n",
    "            }    \n",
    "        with torch.no_grad():\n",
    "            body = bm(**body_parms)\n",
    "        pose_seq_np = body.Jtr.detach().cpu().numpy()\n",
    "        results.append(pose_seq_np)\n",
    "    results = np.concatenate(results,axis=0)\n",
    "    assert results.shape[0] == T\n",
    "    np.save(save_path, results)\n",
    "    return np.isnan(results).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:   0%|                                                                                                            | 0/1620 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1620/1620 [12:15<00:00,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "bad_zip_files = list()\n",
    "unused_files = list()\n",
    "pbar = tqdm(paths,desc='processing',ncols=150)\n",
    "for path in pbar:\n",
    "    save_path = path.replace('./beat2_data', './pose_data')\n",
    "    save_path = save_path[:-3] + 'npy'\n",
    "    if os.path.exists(save_path):\n",
    "        continue\n",
    "    try:\n",
    "        amass_to_pose(path, save_path)\n",
    "    except zipfile.BadZipFile:\n",
    "        bad_zip_files.append(path)\n",
    "    except:\n",
    "        unused_files.append(path)\n",
    "for f in bad_zip_files:\n",
    "    print('bad zip file:',f)\n",
    "for f in unused_files:\n",
    "    print('unused file:',f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment, Mirror and Relocate Motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "bm_params_f = pathlib.Path('./body_models/smplx/SMPLX_NEUTRAL_2020.npz')\n",
    "index_f = pathlib.Path('./index.csv')\n",
    "pose_data_d  = pathlib.Path('./pose_data')\n",
    "joints_d = pathlib.Path('./joints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the corresponding left/right joints from model npy file. We will mirror left/right joints to augment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_params = np.load(bm_params_f,allow_pickle=True)\n",
    "joint2ind = bm_params['joint2num'].item()\n",
    "ind2joint = {v:k\n",
    "             for k,v in joint2ind.items()}\n",
    "l_joints,r_joints = list(),list()\n",
    "for j in joint2ind:\n",
    "    if j.startswith('L_'):\n",
    "        l_j = j\n",
    "        r_j = j.replace('L_','R_')\n",
    "        l_joints.append(joint2ind[l_j])\n",
    "        r_joints.append(joint2ind[r_j])\n",
    "# print('num joints to swap:',len(l_joints))\n",
    "# for l,r in sorted(zip(l_joints,r_joints)):\n",
    "#     print(ind2joint[l],ind2joint[r])\n",
    "joints_to_drop = [joint2ind['Jaw'],\n",
    "                  joint2ind['L_Eye'],\n",
    "                  joint2ind['R_Eye']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirror each file and split to max sequence length of 200 each because MotionGPT uses these limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1620/1620 [01:36<00:00, 16.87it/s]\n"
     ]
    }
   ],
   "source": [
    "data_fs = list(pose_data_d.iterdir())\n",
    "data_fs.sort()\n",
    "pose_data_to_joints_map = ['file,id']\n",
    "for i,f in enumerate(tqdm(data_fs,ncols=150)):\n",
    "    data = np.load(f)\n",
    "    data[...,0] *= -1\n",
    "    data_m = data.copy()\n",
    "    data_m[:,l_joints] = data[:,r_joints]\n",
    "    data_m[:,r_joints] = data[:,l_joints]\n",
    "    data = np.delete(data,joints_to_drop,axis=1)\n",
    "    data_m = np.delete(data_m,joints_to_drop,axis=1)\n",
    "    for j,beg in enumerate(range(0,data.shape[0],200)):\n",
    "        id = f'{i:06}_{j:03}'\n",
    "        id_m = f'M{i:06}_{j:03}'\n",
    "        out_f = joints_d / f'{id}.npy'\n",
    "        out_m_f = joints_d / f'{id_m}.npy'\n",
    "        if out_f.is_file() and out_m_f.is_file():\n",
    "            pose_data_to_joints_map.append(f'{f},{id}')\n",
    "            continue\n",
    "        np.save(out_f,data[beg:beg+200])\n",
    "        np.save(out_m_f,data_m[beg:beg+200])\n",
    "        pose_data_to_joints_map.append(f'{f},{id}')\n",
    "_ = open('pose_data_to_joints_map.txt','w').write('\\n'.join(pose_data_to_joints_map) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write fake text files for each example. Copy a single text file from AMASS dataset to all the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout main -- HumanML3D/texts.zip\n",
    "!rm -rf HumanML3D/texts\n",
    "!unzip -q HumanML3D/texts.zip -d HumanML3D\n",
    "!rm HumanML3D/texts.zip\n",
    "!mv HumanML3D/texts HumanML3D/texts_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating text files: 100%|██████████| 58972/58972 [03:03<00:00, 322.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "texts_d = pathlib.Path('HumanML3D/texts')\n",
    "src_f = pathlib.Path('HumanML3D/texts_orig/000000.txt')\n",
    "texts_d.mkdir()\n",
    "data_fs = list(joints_d.iterdir())\n",
    "data_fs.sort()\n",
    "\n",
    "for f in tqdm(data_fs,'creating text files',ncols=150):\n",
    "    txt_f = f.with_suffix('.txt').name\n",
    "    txt_f = texts_d / txt_f\n",
    "    shutil.copyfile(src_f,txt_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r HumanML3D/texts_orig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shrik_mgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
